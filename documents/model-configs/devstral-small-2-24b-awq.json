{
  "architectures": [
    "Mistral3ForConditionalGeneration"
  ],
  "dtype": "bfloat16",
  "image_token_index": 10,
  "model_type": "mistral3",
  "multimodal_projector_bias": false,
  "projector_hidden_act": "gelu",
  "tie_word_embeddings": false,
  "quantization_config": {
    "config_groups": {
      "group_0": {
        "format": "pack-quantized",
        "input_activations": null,
        "output_activations": null,
        "targets": [
          "Linear"
        ],
        "weights": {
          "actorder": null,
          "block_structure": null,
          "dynamic": false,
          "group_size": 32,
          "num_bits": 4,
          "observer": "mse",
          "observer_kwargs": {},
          "strategy": "group",
          "symmetric": true,
          "type": "int"
        }
      }
    },
    "format": "pack-quantized",
    "global_compression_ratio": null,
    "ignore": [
      "model.vision_tower.transformer.layers.0.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.0.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.0.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.0.attention.k_proj",
      "model.vision_tower.transformer.layers.0.attention.v_proj",
      "model.vision_tower.transformer.layers.0.attention.q_proj",
      "model.vision_tower.transformer.layers.0.attention.o_proj",
      "model.vision_tower.transformer.layers.1.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.1.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.1.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.1.attention.k_proj",
      "model.vision_tower.transformer.layers.1.attention.v_proj",
      "model.vision_tower.transformer.layers.1.attention.q_proj",
      "model.vision_tower.transformer.layers.1.attention.o_proj",
      "model.vision_tower.transformer.layers.2.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.2.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.2.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.2.attention.k_proj",
      "model.vision_tower.transformer.layers.2.attention.v_proj",
      "model.vision_tower.transformer.layers.2.attention.q_proj",
      "model.vision_tower.transformer.layers.2.attention.o_proj",
      "model.vision_tower.transformer.layers.3.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.3.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.3.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.3.attention.k_proj",
      "model.vision_tower.transformer.layers.3.attention.v_proj",
      "model.vision_tower.transformer.layers.3.attention.q_proj",
      "model.vision_tower.transformer.layers.3.attention.o_proj",
      "model.vision_tower.transformer.layers.4.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.4.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.4.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.4.attention.k_proj",
      "model.vision_tower.transformer.layers.4.attention.v_proj",
      "model.vision_tower.transformer.layers.4.attention.q_proj",
      "model.vision_tower.transformer.layers.4.attention.o_proj",
      "model.vision_tower.transformer.layers.5.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.5.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.5.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.5.attention.k_proj",
      "model.vision_tower.transformer.layers.5.attention.v_proj",
      "model.vision_tower.transformer.layers.5.attention.q_proj",
      "model.vision_tower.transformer.layers.5.attention.o_proj",
      "model.vision_tower.transformer.layers.6.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.6.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.6.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.6.attention.k_proj",
      "model.vision_tower.transformer.layers.6.attention.v_proj",
      "model.vision_tower.transformer.layers.6.attention.q_proj",
      "model.vision_tower.transformer.layers.6.attention.o_proj",
      "model.vision_tower.transformer.layers.7.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.7.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.7.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.7.attention.k_proj",
      "model.vision_tower.transformer.layers.7.attention.v_proj",
      "model.vision_tower.transformer.layers.7.attention.q_proj",
      "model.vision_tower.transformer.layers.7.attention.o_proj",
      "model.vision_tower.transformer.layers.8.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.8.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.8.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.8.attention.k_proj",
      "model.vision_tower.transformer.layers.8.attention.v_proj",
      "model.vision_tower.transformer.layers.8.attention.q_proj",
      "model.vision_tower.transformer.layers.8.attention.o_proj",
      "model.vision_tower.transformer.layers.9.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.9.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.9.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.9.attention.k_proj",
      "model.vision_tower.transformer.layers.9.attention.v_proj",
      "model.vision_tower.transformer.layers.9.attention.q_proj",
      "model.vision_tower.transformer.layers.9.attention.o_proj",
      "model.vision_tower.transformer.layers.10.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.10.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.10.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.10.attention.k_proj",
      "model.vision_tower.transformer.layers.10.attention.v_proj",
      "model.vision_tower.transformer.layers.10.attention.q_proj",
      "model.vision_tower.transformer.layers.10.attention.o_proj",
      "model.vision_tower.transformer.layers.11.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.11.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.11.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.11.attention.k_proj",
      "model.vision_tower.transformer.layers.11.attention.v_proj",
      "model.vision_tower.transformer.layers.11.attention.q_proj",
      "model.vision_tower.transformer.layers.11.attention.o_proj",
      "model.vision_tower.transformer.layers.12.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.12.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.12.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.12.attention.k_proj",
      "model.vision_tower.transformer.layers.12.attention.v_proj",
      "model.vision_tower.transformer.layers.12.attention.q_proj",
      "model.vision_tower.transformer.layers.12.attention.o_proj",
      "model.vision_tower.transformer.layers.13.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.13.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.13.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.13.attention.k_proj",
      "model.vision_tower.transformer.layers.13.attention.v_proj",
      "model.vision_tower.transformer.layers.13.attention.q_proj",
      "model.vision_tower.transformer.layers.13.attention.o_proj",
      "model.vision_tower.transformer.layers.14.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.14.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.14.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.14.attention.k_proj",
      "model.vision_tower.transformer.layers.14.attention.v_proj",
      "model.vision_tower.transformer.layers.14.attention.q_proj",
      "model.vision_tower.transformer.layers.14.attention.o_proj",
      "model.vision_tower.transformer.layers.15.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.15.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.15.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.15.attention.k_proj",
      "model.vision_tower.transformer.layers.15.attention.v_proj",
      "model.vision_tower.transformer.layers.15.attention.q_proj",
      "model.vision_tower.transformer.layers.15.attention.o_proj",
      "model.vision_tower.transformer.layers.16.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.16.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.16.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.16.attention.k_proj",
      "model.vision_tower.transformer.layers.16.attention.v_proj",
      "model.vision_tower.transformer.layers.16.attention.q_proj",
      "model.vision_tower.transformer.layers.16.attention.o_proj",
      "model.vision_tower.transformer.layers.17.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.17.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.17.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.17.attention.k_proj",
      "model.vision_tower.transformer.layers.17.attention.v_proj",
      "model.vision_tower.transformer.layers.17.attention.q_proj",
      "model.vision_tower.transformer.layers.17.attention.o_proj",
      "model.vision_tower.transformer.layers.18.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.18.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.18.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.18.attention.k_proj",
      "model.vision_tower.transformer.layers.18.attention.v_proj",
      "model.vision_tower.transformer.layers.18.attention.q_proj",
      "model.vision_tower.transformer.layers.18.attention.o_proj",
      "model.vision_tower.transformer.layers.19.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.19.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.19.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.19.attention.k_proj",
      "model.vision_tower.transformer.layers.19.attention.v_proj",
      "model.vision_tower.transformer.layers.19.attention.q_proj",
      "model.vision_tower.transformer.layers.19.attention.o_proj",
      "model.vision_tower.transformer.layers.20.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.20.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.20.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.20.attention.k_proj",
      "model.vision_tower.transformer.layers.20.attention.v_proj",
      "model.vision_tower.transformer.layers.20.attention.q_proj",
      "model.vision_tower.transformer.layers.20.attention.o_proj",
      "model.vision_tower.transformer.layers.21.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.21.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.21.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.21.attention.k_proj",
      "model.vision_tower.transformer.layers.21.attention.v_proj",
      "model.vision_tower.transformer.layers.21.attention.q_proj",
      "model.vision_tower.transformer.layers.21.attention.o_proj",
      "model.vision_tower.transformer.layers.22.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.22.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.22.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.22.attention.k_proj",
      "model.vision_tower.transformer.layers.22.attention.v_proj",
      "model.vision_tower.transformer.layers.22.attention.q_proj",
      "model.vision_tower.transformer.layers.22.attention.o_proj",
      "model.vision_tower.transformer.layers.23.feed_forward.gate_proj",
      "model.vision_tower.transformer.layers.23.feed_forward.up_proj",
      "model.vision_tower.transformer.layers.23.feed_forward.down_proj",
      "model.vision_tower.transformer.layers.23.attention.k_proj",
      "model.vision_tower.transformer.layers.23.attention.v_proj",
      "model.vision_tower.transformer.layers.23.attention.q_proj",
      "model.vision_tower.transformer.layers.23.attention.o_proj",
      "model.multi_modal_projector.patch_merger.merging_layer",
      "model.multi_modal_projector.linear_1",
      "model.multi_modal_projector.linear_2",
      "lm_head"
    ],
    "kv_cache_scheme": null,
    "quant_method": "compressed-tensors",
    "quantization_status": "compressed",
    "sparsity_config": {},
    "transform_config": {},
    "version": "0.12.3.a20251114"
  },
  "spatial_merge_size": 2,
  "text_config": {
    "attention_dropout": 0.0,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 5120,
    "initializer_range": 0.02,
    "intermediate_size": 32768,
    "max_position_embeddings": 393216,
    "model_type": "ministral3",
    "num_attention_heads": 32,
    "num_hidden_layers": 40,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-05,
    "rope_parameters": {
      "beta_fast": 32.0,
      "beta_slow": 1.0,
      "factor": 48.0,
      "llama_4_scaling_beta": 0.1,
      "mscale": 1.0,
      "mscale_all_dim": 1.0,
      "original_max_position_embeddings": 8192,
      "rope_theta": 100000000.0,
      "rope_type": "yarn",
      "type": "yarn"
    },
    "sliding_window": null,
    "use_cache": true,
    "vocab_size": 131072
  },
  "transformers_version": "5.0.0.dev0",
  "vision_config": {
    "attention_dropout": 0.0,
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_size": 1024,
    "image_size": 1540,
    "initializer_range": 0.02,
    "intermediate_size": 4096,
    "model_type": "pixtral",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "rope_parameters": {
      "rope_theta": 10000.0,
      "rope_type": "default"
    }
  },
  "vision_feature_layer": -1
}