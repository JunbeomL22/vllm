|   1  NVIDIA RTX PRO 6000 Blac...    On  |   00000000:F1:00.0 Off |                    0 |

| N/A   29C    P8             34W /  600W |      14MiB /  97887MiB |      0%      Default |

|                                         |                        |             Disabled |

+-----------------------------------------+------------------------+----------------------+

 

+-----------------------------------------------------------------------------------------+

| Processes:                                                                              |

|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |

|        ID   ID                                                               Usage      |

|=========================================================================================|

|    0   N/A  N/A            3145      G   /usr/lib/xorg/Xorg                        4MiB |

|    1   N/A  N/A            3145      G   /usr/lib/xorg/Xorg                        4MiB |

+-----------------------------------------------------------------------------------------+

user@workstation:~/Projects/local-ai-service$ ./vllm-docker-glm45-air.sh

606834849ddc11402fecf6c78f39b4d0ad3399069e0c625adda16db87868bc5b

user@workstation:~/Projects/local-ai-service$ docker logs -f vllm-server

WARNING 12-25 21:57:44 [argparse_utils.py:195] With `vllm serve`, you should provide the model as a positional argument or in a config file instead of via the `--model` option. The `--model` option will be removed in v0.13.

(APIServer pid=1) INFO 12-25 21:57:44 [api_server.py:1351] vLLM API server version 0.13.0

(APIServer pid=1) INFO 12-25 21:57:44 [utils.py:253] non-default args: {'model_tag': '/models/GLM-4.5-Air-FP8', 'host': '0.0.0.0', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/models/GLM-4.5-Air-FP8', 'max_model_len': 128000, 'served_model_name': ['glm-4.5-air-fp8'], 'attention_backend': 'FLASHINFER', 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'disable_custom_all_reduce': True, 'gpu_memory_utilization': 0.93, 'swap_space': 80.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 16384, 'max_num_seqs': 6, 'enable_chunked_prefill': True}

(APIServer pid=1) INFO 12-25 21:57:44 [model.py:514] Resolved architecture: Glm4MoeForCausalLM

(APIServer pid=1) INFO 12-25 21:57:44 [model.py:1661] Using max model len 128000

(APIServer pid=1) INFO 12-25 21:57:44 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=16384.

(EngineCore_DP0 pid=142) INFO 12-25 21:57:48 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/models/GLM-4.5-Air-FP8', speculative_config=None, tokenizer='/models/GLM-4.5-Air-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=glm-4.5-air-fp8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 8, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}

(EngineCore_DP0 pid=142) WARNING 12-25 21:57:48 [multiproc_executor.py:882] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.

ERROR 12-25 21:57:52 [multiproc_executor.py:751] WorkerProc failed to start.

ERROR 12-25 21:57:52 [multiproc_executor.py:751] Traceback (most recent call last):

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 722, in worker_main

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     worker = WorkerProc(*args, **kwargs)

ERROR 12-25 21:57:52 [multiproc_executor.py:751]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 553, in __init__

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     self.worker.init_device()

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/worker_base.py", line 326, in init_device

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     self.worker.init_device()  # type: ignore

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     ^^^^^^^^^^^^^^^^^^^^^^^^^

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 216, in init_device

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     current_platform.set_device(self.device)

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/vllm/platforms/cuda.py", line 123, in set_device

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     torch.cuda.set_device(device)

ERROR 12-25 21:57:52 [multiproc_executor.py:751]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 567, in set_device

ERROR 12-25 21:57:52 [multiproc_executor.py:751]     torch._C._cuda_setDevice(device)

ERROR 12-25 21:57:52 [multiproc_executor.py:751] torch.AcceleratorError: CUDA error: uncorrectable ECC error encountered

ERROR 12-25 21:57:52 [multiproc_executor.py:751] Search for `cudaErrorECCUncorrectable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.

ERROR 12-25 21:57:52 [multiproc_executor.py:751] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.

ERROR 12-25 21:57:52 [multiproc_executor.py:751] For debugging consider passing CUDA_LAUNCH_BLOCKING=1

ERROR 12-25 21:57:52 [multiproc_executor.py:751] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

ERROR 12-25 21:57:52 [multiproc_executor.py:751]

INFO 12-25 21:57:52 [multiproc_executor.py:709] Parent process exited, terminating worker

INFO 12-25 21:57:52 [multiproc_executor.py:709] Parent process exited, terminating worker

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866] EngineCore failed to start.

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866] Traceback (most recent call last):

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 637, in __init__

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     super().__init__(

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 102, in __init__

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     self.model_executor = executor_class(vllm_config)

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     super().__init__(vllm_config)

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     self._init_executor()

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     self.workers = WorkerProc.wait_for_ready(unready_workers)

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 660, in wait_for_ready

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866]     raise e from None

(EngineCore_DP0 pid=142) ERROR 12-25 21:57:53 [core.py:866] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.

(EngineCore_DP0 pid=142) Process EngineCore_DP0:

(EngineCore_DP0 pid=142) Traceback (most recent call last):

(EngineCore_DP0 pid=142)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap

(EngineCore_DP0 pid=142)     self.run()

(EngineCore_DP0 pid=142)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run

(EngineCore_DP0 pid=142)     self._target(*self._args, **self._kwargs)

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 870, in run_engine_core

(EngineCore_DP0 pid=142)     raise e

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 857, in run_engine_core

(EngineCore_DP0 pid=142)     engine_core = EngineCoreProc(*args, **kwargs)

(EngineCore_DP0 pid=142)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 637, in __init__

(EngineCore_DP0 pid=142)     super().__init__(

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 102, in __init__

(EngineCore_DP0 pid=142)     self.model_executor = executor_class(vllm_config)

(EngineCore_DP0 pid=142)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__

(EngineCore_DP0 pid=142)     super().__init__(vllm_config)

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__

(EngineCore_DP0 pid=142)     self._init_executor()

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor

(EngineCore_DP0 pid=142)     self.workers = WorkerProc.wait_for_ready(unready_workers)

(EngineCore_DP0 pid=142)                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(EngineCore_DP0 pid=142)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py", line 660, in wait_for_ready

(EngineCore_DP0 pid=142)     raise e from None

(EngineCore_DP0 pid=142) Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.

(APIServer pid=1) Traceback (most recent call last):

(APIServer pid=1)   File "/usr/local/bin/vllm", line 10, in <module>

(APIServer pid=1)     sys.exit(main())

(APIServer pid=1)              ^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main

(APIServer pid=1)     args.dispatch_function(args)

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd

(APIServer pid=1)     uvloop.run(run_server(args))

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 96, in run

(APIServer pid=1)     return __asyncio.run(

(APIServer pid=1)            ^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run

(APIServer pid=1)     return runner.run(main)

(APIServer pid=1)            ^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run

(APIServer pid=1)     return self._loop.run_until_complete(task)

(APIServer pid=1)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 48, in wrapper

(APIServer pid=1)     return await main

(APIServer pid=1)            ^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1398, in run_server

(APIServer pid=1)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 1417, in run_server_worker

(APIServer pid=1)     async with build_async_engine_client(

(APIServer pid=1)                ^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__

(APIServer pid=1)     return await anext(self.gen)

(APIServer pid=1)            ^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client

(APIServer pid=1)     async with build_async_engine_client_from_engine_args(

(APIServer pid=1)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__

(APIServer pid=1)     return await anext(self.gen)

(APIServer pid=1)            ^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py", line 213, in build_async_engine_client_from_engine_args

(APIServer pid=1)     async_llm = AsyncLLM.from_vllm_config(

(APIServer pid=1)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 215, in from_vllm_config

(APIServer pid=1)     return cls(

(APIServer pid=1)            ^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 134, in __init__

(APIServer pid=1)     self.engine_core = EngineCoreClient.make_async_mp_client(

(APIServer pid=1)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client

(APIServer pid=1)     return AsyncMPClient(*client_args)

(APIServer pid=1)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 820, in __init__

(APIServer pid=1)     super().__init__(

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 477, in __init__

(APIServer pid=1)     with launch_core_engines(vllm_config, executor_class, log_stats) as (

(APIServer pid=1)          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

(APIServer pid=1)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__

(APIServer pid=1)     next(self.gen)

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 903, in launch_core_engines

(APIServer pid=1)     wait_for_engine_startup(

(APIServer pid=1)   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup

(APIServer pid=1)     raise RuntimeError(